---
title: "Homework3"
author: "Ethan Ashby"
date: "2/19/2020"
output:
  pdf_document: default
  html_document: default
---

```{r, echo=FALSE, message=FALSE, results='hide', cache=TRUE}
library(mosaic)
library(infer)
library(tidyverse)
library(MASS)

#run f test for linearity
formal.test.linearity <- function(x,y,digits) {
  # Conducts a formal test of linearity by `binning' x values to produce psuedo-replications
  #
  # Args: 
  #  x: predictor variable
  #  y: repsons variable
  #  digits: number of digits used in the rounding - for instance digits=1 takes 47 to 50 
  # Returns:
  #  ANOVA table (SSs, dfs, F-statistic, p-value) for model comparison
  x.round <- round(x,digits=-digits)
  if (length(unique(x.round))==length(x))
    stop('rounding not sufficient to produce repeated values of x')
  fit1 <- lm(y ~ x.round)
  fit2 <- lm(y~as.factor(x.round))
  return(anova(fit1,fit2))
}
```

```{r, echo=FALSE, message=FALSE, results='hide', cache=TRUE}
data<-read.csv("Hw3_data.csv", header=FALSE)
```

We begin by fitting a model to predict number of physicians using the estimated population.
```{r, echo=FALSE, cache=TRUE, fig.keep='none', fig.height=2.5, fig.width=6, fig.align='center'}
#Fit a linear model that predicts number of active physicians (V4) using estimated population (V8)
fit<-lm(V7~V4, data=data)
xyplot(V7~V4, data=data, type=c("r", "p"))
```
We use the box-cox procedure to inform possible data transformations. The log-likelihood is maximized at around $\lambda$=0, which suggests a log transformation of the y variable (number of physicians). When we refit and rerun box cox, we also see a log transform of the x variable is suggested
```{r, echo=FALSE, fig.keep='none', cache=TRUE, fig.height=3.5, fig.width=3.5}
#use box cox method to inform data transformation, y variable needs log transform
boxcox(fit)
#refit, we want to transform our explanatory variable x, so we're going to flip explanatory predictor relationship
fit_1<-lm(V4~V7, data=data)
#suggests a log transform of x as well
boxcox(fit_1)
```

```{r, echo=TRUE, cache=TRUE, fig.height=2.75, fig.width=6, fig.align="center"}
#build model predicting number of physicians using log(pop)
new_fit<-lm(log(V7)~log(V4), data=data)
xyplot(log(V7)~log(V4), data=data, type=c("r", "p"))
#this looks a lot better
```

```{r, echo=FALSE, cache=TRUE, fig.height=3.5, fig.width=3.5}
#diagnostic plots
plot(new_fit)
```
Our diagnostic plots spell pretty good news for our updated model. The residuals vs fitted plot shows that the residuals show no trend with the fitted values, indicating constant variance and linearity are upheld in our model. Our Q-Q plot shows that despite a few outliers, that the majority of the residuals are normally distributed, which supports another of our important regression hypotheses. The scale location plot shows the standardized residuals are spread evenly above and below the line, supporting homoskedasticity. Our residuals vs leverage plot shows that there are no influential points that could be messing up our regression.
```{r}
#rounding to 100ths digit leads to solid representation of my data
#xyplot(log(V7)~round(log(V4), 2), data=data, type=c("r", "p"))
formal.test.linearity(log(data$V4), log(data$V7), digits=-2)
```
And our p-value is very large, so we fail to reject the null hypothesis of linearity.

```{r, echo=FALSE}
#Confidence interval for county with 1 million people
new_data<-data.frame(V4=1000000)
exp(predict(new_fit, new_data, interval="confidence")[c(1,3)])

#prediction interval for county with 1 million people
exp(predict(new_fit, new_data, interval="predict")[c(1,3)])
```
Confidence interval for a county of 1 million people is [2678.735, 2939.674]. Our prediction interval for a county of 1 million people is [2678.735, 7240.686].


